---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: t5
spec:
  storageClassName: "standard-rwo"
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  claimRef:
    namespace: default
    name: t5
  csi:
    driver: pd.csi.storage.gke.io
    volumeHandle: projects/lustrous-baton-363720/regions/us-central1/disks/t5
    fsType: ext4
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  namespace: default
  name: t5
spec:
  storageClassName: "standard-rwo"
  volumeName: t5
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prepare-scripts
data:
  checkout.sh: >
    #!/bin/sh

    REPO_DIR="${REPO_DIR:-./}"

    if test -n "$REMOTE_REPO"; then
      if ! git clone "$REMOTE_REPO" "$REPO_DIR"; then
        echo "Skipping."
      fi
    else
      echo "REMOTE_REPO env var is empty"
    fi

  mar.sh: >
    #!/bin/sh

    MODEL_SRC="/home/t5/model"
    MODEL_HANDLER_SRC="/home/t5/handler/gpu-workload/t5/model"
    MODEL_STORE_DIR="/home/model-server/model-store"
    MAR_FILE="$MODEL_STORE_DIR/$MODEL_NAME.mar"

    if ! test -f "$MAR_FILE"; then
      torch-model-archiver \
        --model-name="$MODEL_NAME" \
        --version="$MODEL_VERSION" \
        --model-file="$MODEL_HANDLER_SRC/model.py" \
        --handler="$MODEL_HANDLER_SRC/handler.py" \
        --serialized-file="$MODEL_SRC/pytorch_model.bin" \
        --extra-files="$MODEL_SRC/config.json,$MODEL_SRC/spiece.model,$MODEL_SRC/tokenizer.json,$MODEL_HANDLER_SRC/setup_config.json" \
        --runtime="python" \
        --export-path="$MODEL_STORE_DIR" \
        --requirements-file="$MODEL_HANDLER_SRC/requirements.txt"
    fi

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: torchserve-files
data:
  start-torchserve.sh: |
    #!/bin/sh

    export PATH="/home/model-server/.local/bin:$PATH"
    torchserve --start --foreground

  config.properties: |
    inference_address=http://0.0.0.0:8080
    management_address=http://0.0.0.0:8081
    metrics_address=http://0.0.0.0:8082
    install_py_dep_per_model=true
    number_of_netty_threads=32
    job_queue_size=1000
    workflow_store=/home/model-server/wf-store
    model_store=/home/model-server/model-store
    load_models=all
